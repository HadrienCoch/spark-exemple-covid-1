# Importation des biblioth√®ques n√©cessaires
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, avg, sum as spark_sum, row_number
from pyspark.sql.types import DoubleType
from pyspark.sql.window import Window
from dotenv import load_dotenv
import os

# Chargement des variables d'environnement depuis le fichier .env
load_dotenv()

DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT")
DB_NAME = os.getenv("DB_NAME")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")

if not all([DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD]):
    raise ValueError("Une ou plusieurs variables d'environnement sont manquantes dans le fichier .env.")

# Initialisation de la session Spark avec le driver PostgreSQL
spark = SparkSession.builder \
    .appName("ETL_COVID19_Postgres") \
    .config("spark.jars", "../postgresql-42.7.5.jar") \
    .getOrCreate()

# Extraction des donn√©es √† partir des fichiers CSV
vaccination_data = spark.read.option("header", "true").csv("data/data_covid/vaccination-data.csv")
covid_global_data = spark.read.option("header", "true").csv("data/data_covid/WHO-COVID-19-global-data.csv")

# Conversion de la colonne 'Date_reported' en type date
covid_global_data = covid_global_data.withColumn("Date_reported", to_date(col("Date_reported"), "yyyy-MM-dd"))

# Suppression des doublons et gestion des valeurs manquantes pour les donn√©es de vaccination
vaccination_data = vaccination_data.dropDuplicates().fillna({
    "PERSONS_VACCINATED_1PLUS_DOSE": 0,
    "PERSONS_LAST_DOSE": 0
})

# Remplacement des valeurs manquantes dans les donn√©es COVID-19 globales
covid_global_data = covid_global_data.fillna({
    "New_cases": 0,
    "New_deaths": 0,
    "Cumulative_cases": 0
})

# Conversion des colonnes en types num√©riques (Double) pour permettre les calculs
vaccination_data = vaccination_data.withColumn("PERSONS_VACCINATED_1PLUS_DOSE", col("PERSONS_VACCINATED_1PLUS_DOSE").cast(DoubleType()))
vaccination_data = vaccination_data.withColumn("PERSONS_LAST_DOSE", col("PERSONS_LAST_DOSE").cast(DoubleType()))
covid_global_data = covid_global_data.withColumn("New_cases", col("New_cases").cast(DoubleType()))
covid_global_data = covid_global_data.withColumn("New_deaths", col("New_deaths").cast(DoubleType()))
covid_global_data = covid_global_data.withColumn("Cumulative_cases", col("Cumulative_cases").cast(DoubleType()))

# Agr√©gation des donn√©es COVID-19
# D√©finition d'une fen√™tre pour s√©lectionner la derni√®re entr√©e (la plus r√©cente) par pays en fonction de la date
window_spec = Window.partitionBy("Country").orderBy(col("Date_reported").desc())

# S√©lection de la derni√®re entr√©e pour chaque pays concernant les cas cumul√©s
latest_covid_data = covid_global_data.withColumn("row_num", row_number().over(window_spec)) \
                                      .filter(col("row_num") == 1) \
                                      .drop("row_num")

# Calcul de la moyenne (moyenne des valeurs cumulatives) pour les personnes vaccin√©es
# Renommage en total_cumulative_people_vaccinated et total_cumulative_people_fully_vaccinated pour refl√©ter le contenu
vaccination_summary = vaccination_data.groupBy("COUNTRY") \
    .agg(
        avg("PERSONS_VACCINATED_1PLUS_DOSE").alias("total_cumulative_people_vaccinated"),
        avg("PERSONS_LAST_DOSE").alias("total_cumulative_people_fully_vaccinated")
    )

# Calcul des moyennes des nouveaux cas et des nouveaux d√©c√®s par pays
covid_summary = covid_global_data.groupBy("Country") \
    .agg(
        avg("New_cases").alias("avg_new_cases"),
        avg("New_deaths").alias("avg_new_deaths")
    )

# Calcul du total des cas cumul√©s par pays en utilisant les derni√®res donn√©es disponibles
covid_cumulative_summary = latest_covid_data.groupBy("Country") \
    .agg(spark_sum("Cumulative_cases").alias("total_cumulative_cases"))

# üî• R√©cup√©ration du total mondial des cas cumul√©s
global_cumulative_cases = covid_cumulative_summary.agg(
    spark_sum("total_cumulative_cases").alias("global_total_cumulative_cases")
)

# Configuration de la connexion JDBC pour PostgreSQL
postgres_url = f"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}"
postgres_properties = {
    "user": DB_USER,
    "password": DB_PASSWORD,
    "driver": "org.postgresql.Driver"
}

# ‚úÖ Insertion des donn√©es dans "global_total_cumulative_cases"
global_cumulative_cases.write \
    .jdbc(url=postgres_url, table="global_total_cumulative_cases", mode="overwrite", properties=postgres_properties)

print("üìä Total des cas cumul√©s ins√©r√© dans global_total_cumulative_cases avec succ√®s !")

# Fusion des agr√©gats COVID-19 : nouveaux cas, nouveaux d√©c√®s et cas cumul√©s
covid_combined_summary = covid_summary.join(
    covid_cumulative_summary,
    covid_summary.Country == covid_cumulative_summary.Country,
    "inner"
).select(
    covid_summary.Country,
    "avg_new_cases",
    "avg_new_deaths",
    "total_cumulative_cases"
)

# Jointure entre les donn√©es de vaccination et les donn√©es COVID-19 fusionn√©es
# Ici, nous conservons √©galement la colonne total_cumulative_cases dans global_statistics
combined_data = vaccination_summary.join(
    covid_combined_summary,
    vaccination_summary.COUNTRY == covid_combined_summary.Country,
    "inner"
).select(
    vaccination_summary.COUNTRY.alias("Country"),
    "total_cumulative_people_vaccinated",
    "total_cumulative_people_fully_vaccinated",
    "avg_new_cases",
    "avg_new_deaths",
    "total_cumulative_cases"
)

# ‚úÖ Insertion des donn√©es fusionn√©es dans la table "global_statistics"
combined_data.write \
    .jdbc(url=postgres_url, table="global_statistics", mode="overwrite", properties=postgres_properties)

print("‚úÖ Donn√©es ins√©r√©es avec succ√®s dans la table global_statistics.")

# Fermeture de la session Spark pour lib√©rer les ressources
spark.stop()
