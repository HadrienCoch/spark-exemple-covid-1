from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, max as spark_max, sum as spark_sum, when
from pyspark.sql.types import DoubleType
from dotenv import load_dotenv
import os

# üîí Chargement des variables d'environnement
load_dotenv()

DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT")
DB_NAME = os.getenv("DB_NAME")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")

if not all([DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD]):
    raise ValueError("‚ùå Une ou plusieurs variables d'environnement sont manquantes dans le fichier .env.")

# üöÄ Initialisation de la session Spark
spark = SparkSession.builder \
    .appName("ETL_COVID19_Postgres") \
    .config("spark.jars", "../postgresql-42.7.5.jar") \
    .getOrCreate()

# 1Ô∏è‚É£ EXTRACTION
vaccination_data = spark.read.option("header", "true").csv("data/data_covid/vaccination-data.csv")
covid_global_data = spark.read.option("header", "true").csv("data/data_covid/WHO-COVID-19-global-data.csv")

# 2Ô∏è‚É£ TRANSFORMATION
covid_global_data = covid_global_data.withColumn("Date_reported", to_date(col("Date_reported"), "yyyy-MM-dd"))
vaccine_start_date = "2021-01-01"

# ‚û°Ô∏è Segmentation avant/apr√®s vaccination
covid_global_data = covid_global_data.withColumn(
    "vaccine_period",
    when(col("Date_reported") < vaccine_start_date, "Before Vaccine").otherwise("After Vaccine")
)

# ‚û°Ô∏è Remplissage des valeurs manquantes
covid_global_data = covid_global_data.fillna({
    "New_cases": 0,
    "New_deaths": 0,
    "Cumulative_cases": 0,
    "WHO_region": "OTHER"
})

# ‚û°Ô∏è Conversion des colonnes
covid_global_data = covid_global_data.withColumn("New_cases", col("New_cases").cast(DoubleType()))
covid_global_data = covid_global_data.withColumn("New_deaths", col("New_deaths").cast(DoubleType()))
covid_global_data = covid_global_data.withColumn("Cumulative_cases", col("Cumulative_cases").cast(DoubleType()))

# 3Ô∏è‚É£ AGR√âGATION corrig√©e
covid_region_summary = covid_global_data.groupBy("WHO_region", "vaccine_period") \
    .agg(
        spark_max("Cumulative_cases").alias("total_cumulative_cases"),  # ‚úÖ Correction ici
        spark_sum("New_cases").alias("total_new_cases"),
        spark_sum("New_deaths").alias("total_new_deaths")
    )

# 4Ô∏è‚É£ CONNEXION PostgreSQL
postgres_url = f"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}"
postgres_properties = {
    "user": DB_USER,
    "password": DB_PASSWORD,
    "driver": "org.postgresql.Driver"
}

# 5Ô∏è‚É£ INSERTION DES DONN√âES
covid_region_summary.write \
    .jdbc(url=postgres_url, table="covid_region_summary", mode="overwrite", properties=postgres_properties)

print("‚úÖ Donn√©es r√©gionales corrig√©es ins√©r√©es avec succ√®s dans PostgreSQL !")

# 6Ô∏è‚É£ Fermeture de Spark
spark.stop()